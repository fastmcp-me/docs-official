---
title: Agent Flow
description: Learn how to measure the correctness and coherence of an agentic trajectory by validating it against user-specified natural language tests
---

import { DefinitionCard } from "/snippets/components/definition-card.mdx";

import SnippetPrompt from "/snippets/prompts/metrics/agent-flow.mdx"

**Agent flow** is a custom LLM-as-a-judge session-level metric, based around a pre-created prompt available from Galileo with user customizations.

<DefinitionCard>
<strong>Agent Flow</strong> is a binary evaluation metric that measures the correctness and coherence of an agentic trajectory by validating it against user-specified natural language tests.
</DefinitionCard>

A trajectory is said to pass the Agent Flow metric if and only if all the user-defined natural language conditions are successfully satisfied by the agent's realized behavior or output.

<Note>
When you create this metric, you will need to provide the flow conditions in the prompt.
</Note>

This is a **boolean** metric, returning either 0% (false) or 100% (true) - 0% means the agent is not correct based on **all** the user defined tests, 100% means it is correct. If you use multiple judges, then the score will be a percentage based on the number of judges who scored true vs false. For example, if 4 out of 5 scored the metric as true, the score would be 80%.

## Create the agent flow metric

This metric needs to be manually created, using a prompt defined by Galileo.

<Steps>
<Step title="Create a new LLM-as-a-judge metric">
Create a new LLM-as-a-judge metric by following the instructions in our [LLM-as-a-judge concept guide](/concepts/metrics/custom-metrics/custom-metrics-ui-llm).

Use the following settings:

| Setting           | Value |
| :---------------- | ----- |
| Name              | `Agent flow` |
| LLM Model         | Select your preferred model |
| Apply to          | **Session** |
| Advanced Settings | Configure these as required for your needs |

</Step>
<Step title="Set the prompt">

Set the prompt to the following:

<SnippetPrompt/>

</Step>
<Step title="Customize the prompt by adding your user-defined tests">

This prompt needs to be customized based on your application, and the inputs and outputs you are expecting. Replace `{{ Add your tests here }}` with a numbered list of tests in natural language that can be used to evaluate the agent efficiency. This can include:

- Expected tool or agent calls, using the tool or agent names
- Conditions on tool or agent calling (e.g. if tool x is called, don't call agent y)
- Expectations around the input or output parameters to tools and agents
- Limitations on the number of tool or agent calls

For example, imagine you were creating an agent to provide advice on exercises for different body parts, such as for a physical therapy application. This has multiple tools, including `list_by_target_muscle_for_exercised`, `list_by_body_part_for_exercised`, `list_of_bodyparts_for_exercised`. Some user tests might be:

{/*<!-- markdownlint-disable MD013 -->*/}

```output wrap
1. If a call to "list_by_target_muscle_for_exercised" returns an error that contains the text "target not found", the agent should subsequently attempt an alternative lookup by calling either "list_by_body_part_for_exercised" or "list_of_bodyparts_for_exercised"
2. When the user asks for exercises that target leg muscles, the agent must call at least one of the tools ["list_by_target_muscle_for_exercised", "list_by_body_part_for_exercised"] during the conversation
3. After receiving a successful response from "list_by_body_part_for_exercised", the agent's following natural-language message must contain at least one exercise name, the corresponding equipment, and an animated demonstration URL taken from the tool output
4. Every invocation of the tool "list_by_body_part_for_exercised" must include the required parameter "bodypart"
5. After receiving data from list_by_body_part_for_exercised, the agent response must include the exercise id for every exercise it presents to the user
6. No assistant message should include more than one tool invocation
7. The agent should conclude the conversation with a human-readable answer that summarizes the requested leg exercises using data returned from the tools
```

{/*<!-- markdownlint-enable MD013 -->*/}

</Step>

<Step title="Save the metric">
Save the metric, then turn it on for your Log stream.
</Step>
</Steps>

Your metric is now ready to use in your project.
