---
title: Model Confidence Metrics
description: Understand your AI's certainty in its responses with Galileo's model confidence metrics
---
import ModelConfidenceMetrics from "/snippets/components/Model-Confidence-Metrics.mdx";

Model confidence metrics help you gauge how certain your AI is about its answers. These metrics are useful for flagging uncertain responses, improving reliability, and knowing when to involve a human in the loop.

Use these metrics when you want to:
- Identify responses where the model is unsure or likely to make mistakes.
- Improve user trust by surfacing confidence scores or warnings.
- Analyze which prompts or situations are most challenging for your AI.

Below is a quick reference table of all model confidence metrics:

<ModelConfidenceMetrics />

---

## Next steps

- [Back to Metrics Overview](/concepts/metrics/overview)
- [Compare all metrics](/concepts/metrics/metric-comparison)
