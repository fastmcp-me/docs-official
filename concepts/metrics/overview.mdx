---
title: Metrics Overview
description: Explore Galileo's comprehensive metrics framework for evaluating and improving AI system performance across multiple dimensions
---

import ResponseQuality from "/snippets/components/Response-Quality-Metrics.mdx";
import SafetyAndComplianceMetrics from "/snippets/components/Safety-And-Compliance-Metrics.mdx";
import AgenticPerformanceMetrics from "/snippets/components/Agentic-Performance-Metrics.mdx";
import ModelConfidenceMetrics from "/snippets/components/Model-Confidence-Metrics.mdx";
import ExpressionAndReadabilityMetrics from "/snippets/components/Expression-And-Readability-Metrics.mdx";

Galileo comes with a set of ready to use [out-of-the-box metrics](#out-of-the-box-metric-categories) that allow you to see how your AI is performing. With these metrics, you can quickly spot problems, track improvements, and make your AI work better for your users. These metrics [apply to different node types](#supported-node-types) (such as session, trace, or different span types), depending on the metric.

You can then expand these metrics with custom metrics, creating using [LLM-as-a-judge](/concepts/metrics/custom-metrics/custom-metrics-ui-llm), or [custom code-based metrics](/concepts/metrics/custom-metrics/custom-metrics-ui-code).

To calculate out-of-the-box metrics, or LLM-as-a-judge metrics, you will need to configure an integration with an LLM, or to [Luna-2](/concepts/luna/luna). Connect Galileo to your language model by adding your API key on the [integrations page](https://app.galileo.ai/settings/integrations) from within the Galileo application.

You can improve the metric calculation based on your requirements using [continuous learning via human feedback (CLHF)](/concepts/metrics/custom-metrics/continuous-learning-via-human-feedback). This allows you to continuously provide feedback in natural language that automatically improves the metrics to align better with your domain, or expected inputs and outputs.

Metrics can be used with [experiments](/concepts/experiments/running-experiments-in-console#step-5-add-metrics), and [Log streams](/concepts/logging/configure-metrics/configure-metrics).

## Configure Galileo for out-of-the-box and LLM-as-a-judge metrics

LLM-based metrics use an LLM to evaluate inputs and outputs. To use these metrics from Log streams or experiments, you need to configure an integration with an LLM platform.

<Steps>
<Step title="Open the integrations page">
Select the user menu in the top right, then select **Integrations**.

<Columns cols={2}>
![The integrations menu](/concepts/metrics/integrations/user-menu-integrations.webp)
</Columns>
</Step>
<Step title="Add an integration">
Locate the option for the LLM platform you are using, then select the **+Add Integration** button.

![The add integration button](/concepts/metrics/integrations/openai-integration-selected.webp)
</Step>
<Step title="Add the settings">

Set the relevant settings for your integration, such as your API keys or endpoints. Then select **Save**.

<Columns cols={2}>
![The OpenAI integrations pane](/concepts/metrics/integrations/openai-integration-api-key.webp)
</Columns>
</Step>
</Steps>

## Using metrics effectively

To get the most value from Galileo's metrics:

1. **Start with key metrics** - Focus on metrics most relevant to your use case
1. **Establish baselines** - Understand your current performance before making changes
1. **Track trends over time** - Monitor how metrics change as you iterate on your system
1. **Combine multiple metrics** - Look at related metrics together for a more complete picture
1. **Set thresholds** - Define acceptable ranges for critical metrics
1. **Improve the metrics** - Use CLHF to continuously improve the metrics

## Out-of-the-Box metric categories

Our metrics can be broken down into five key categories, each addressing a specific aspect of AI system performance. Many times, folks benefit from using metrics from more than one category, depending on the metrics that matter most to them.  Galileo also supports [custom metrics](/concepts/metrics/custom-metrics/custom-metrics-ui-llm) that are able to be implemented alongside the out-of-the-box metric options.

| Category | Description | When to Use | Example Use Cases |
|:---------|:------------|:------------|:------------------|
| [Agentic](/concepts/metrics/overview#agentic-performance-metrics) | Metrics that evaluate how effectively AI agents perform tasks, use tools, and progress toward goals. | When building and optimizing AI systems that take actions, make decisions, or use tools to accomplish tasks. | <ul><li>Evaluating a travel planning agent's ability to book complete itineraries</li><li>Assessing a coding assistant's appropriate use of APIs and libraries</li><li>Measuring a data analysis agent's tool selection effectiveness</li></ul> |
| [Expression and Readability](/concepts/metrics/overview#expression-and-readability-metrics) | Metrics that evaluate the style, tone, clarity, and overall presentation of AI-generated content. | When the format, tone, and presentation of AI outputs are important for user experience or brand consistency. | <ul><li>Ensuring a luxury brand chatbot maintains a sophisticated tone</li><li>Verifying educational content is presented at the appropriate reading level</li><li>Measuring clarity and conciseness in technical documentation generation</li></ul> |
| [Model Confidence](/concepts/metrics/overview#model-confidence-metrics) | Metrics that measure how certain or uncertain your AI model is about its responses. | When you want to flag low-confidence responses for review, improve system reliability, or better understand model uncertainty. | <ul><li>Flagging uncertain answers in a customer support chatbot for human review</li><li>Identifying low-confidence predictions in a medical diagnosis assistant</li><li>Improving user trust by surfacing confidence scores in AI-generated content</li></ul> |
| [Response Quality](/concepts/metrics/overview#response-quality-metrics) | Metrics that assess the accuracy, completeness, relevance, and overall quality of AI-generated responses. | When evaluating how well AI systems answer questions, follow instructions, or provide information based on context. | <ul><li>Measuring factual accuracy in a medical information system</li><li>Evaluating how well a RAG system uses retrieved information</li><li>Assessing if customer service responses address all parts of a query</li></ul> |
| [Safety and Compliance](/concepts/metrics/overview#safety-and-compliance-metrics) | Metrics that identify potential risks, harmful content, bias, or privacy concerns in AI interactions. | When ensuring AI systems meet regulatory requirements, protect user privacy, and avoid generating harmful or biased content. | <ul><li>Detecting PII in healthcare chatbot conversations</li><li>Identifying potential prompt injection attacks in public-facing systems</li><li>Measuring bias in hiring or loan approval recommendation systems</li></ul> |

### Agentic performance metrics

Use these metrics to evaluate how well your AI agents use tools, make decisions, and accomplish multi-step tasks. They're a good fit when you're building agents that need to interact with external systems or complete complex workflows.

<AgenticPerformanceMetrics />

### Expression and readability metrics

Use these metrics to assess the style, tone, and clarity of your AI's generated content. They're helpful when you want your AI to communicate clearly, match your brand's voice, or produce content that's easy for users to understand.

<ExpressionAndReadabilityMetrics />

### Model confidence metrics

Use these metrics to understand how certain or uncertain your AI model is about its answers. They're useful when you want to flag low-confidence responses for review or improve your system's reliability.

<ModelConfidenceMetrics />

### Response quality metrics

Use these metrics to evaluate how well your AI answers user questions and follows instructions. They're especially helpful when you want to ensure your system is providing accurate, complete, and relevant responses.

<ResponseQuality />

### Safety and compliance metrics

Use these metrics to identify potential risks, harmful content, or compliance issues in your AI's responses. They're important when you need to protect users, meet regulatory requirements, or avoid generating biased or unsafe content.

<SafetyAndComplianceMetrics />

## Supported node types

Different metrics apply to different node types, such as sessions, traces, or spans.

<AccordionGroup>
<Accordion title="Session">
- [Action completion](/concepts/metrics/agentic/action-completion)
- [Agent efficiency](/concepts/metrics/agentic/agent-efficiency)
- [Agent flow](/concepts/metrics/agentic/agent-flow)
- [Conversation quality](/concepts/metrics/agentic/conversation-quality) (Trace input/outputs only)
- [User Intent change](/concepts/metrics/agentic/intent-change) (Trace input/outputs only)
</Accordion>
<Accordion title="Trace">
- [Action advancement](/concepts/metrics/agentic/action-advancement)
- [Ground Truth Adherence](/concepts/metrics/response-quality/ground-truth-adherence)
- [PII / CPNI / PHI](/concepts/metrics/safety-and-compliance/pii) (Root input/output only)
- [Prompt Injection](/concepts/metrics/safety-and-compliance/prompt-injection)
- [Sexism / Bias](/concepts/metrics/safety-and-compliance/sexism) (Root input/output only)
- [Tone](/concepts/metrics/expression-and-readability/tone) (Root input/output only)
- [Toxicity](/concepts/metrics/safety-and-compliance/toxicity) (Root input/output only)
</Accordion>
<Accordion title="LLM Span">
- [BLEU & ROUGE](/concepts/metrics/expression-and-readability/bleu-and-rouge)
- [Completeness](/concepts/metrics/response-quality/completeness)
- [Context Adherence](/concepts/metrics/response-quality/context-adherence)
- [Correctness (factuality)](/concepts/metrics/response-quality/correctness)
- [Instruction Adherence](/concepts/metrics/response-quality/instruction-adherence)
- [Prompt Perplexity](/concepts/metrics/model-confidence/prompt-perplexity)
- [Tool selection quality](/concepts/metrics/agentic/tool-selection-quality)
- [Uncertainty](/concepts/metrics/model-confidence/uncertainty)
</Accordion>
<Accordion title="Retriever span">
- [Chunk Attribution Utilization](/concepts/metrics/response-quality/chunk-attribution)
- [Context Relevance (Query Adherence)](/concepts/metrics/response-quality/context-relevance)
</Accordion>
<Accordion title="Tool span">
- [Tool error](/concepts/metrics/agentic/tool-error)
</Accordion>
</AccordionGroup>

## Next steps

<CardGroup cols={2}>
<Card title="Custom LLM-as-a-judge metrics" icon="brain" horizontal href="/concepts/metrics/custom-metrics/custom-metrics-ui-llm">
    Learn how to create evaluation metrics using LLMs to judge the quality of responses
</Card>
<Card title="Custom code-based metrics" icon="code" horizontal href="/concepts/metrics/custom-metrics/custom-metrics-ui-code">
    Learn how to create, register, and use custom metrics to evaluate your LLM applications
</Card>
<Card title="Customizing Your LLM-Powered Metrics via CLHF" icon="message" horizontal href="/concepts/metrics/custom-metrics/continuous-learning-via-human-feedback">
    Learn how to customize your LLM-powered metrics with Continuous Learning via Human Feedback
</Card>
</CardGroup>
