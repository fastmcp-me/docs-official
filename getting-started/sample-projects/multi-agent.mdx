---
title: Multi-agent banking chatbot sample
description: Get started with the multi-agent banking chatbot sample project powered by LangGraph, with RAG using Pinecone
---

The multi-agent banking chatbot sample project is a demo of a multi-agent chatbot powered by LangGraph, with RAG using Pinecone as a vector database. You can have a conversation with the chatbot, and it will bring back information on your (fictional) credit score, as well as give you details of credit cards available from a fictional bank.

<CardGroup cols={1}>
<Card title="Get the code" icon="code" href="https://github.com/rungalileo/sdk-examples" horizontal>
  Get the code for the sample project. You can find this project by cloning the Galileo SDK Examples repo.

The code for this project is in the `/python/agent/langgraph-fsi-agent/after/` or `/typescript/agent/langgraph-fsi-agent/` folder.
</Card>
</CardGroup>

![The project page for the multi-agent banking chatbot with setup instructions and an insight](/getting-started/sample-projects/multi-agent-project-page.webp)

The code for this sample is available in Python and TypeScript, and you can run this code to generate more traces, and experiment with improving the app based off the evaluations.

The Python version of this app uses Chainlit to host the chatbot in a web UI. The TypeScript version is terminal-based.

## Evaluate the app

The sample project comes with a Log stream pre-populated with a set of traces for some sample interactions with the chatbot - some asking relevant questions, some asking questions unrelated to the banking agents capabilities.

### Investigate the Log stream

Navigate to the **Default Log Stream** by ensuring it is selected, and selecting the **View all logs** button.

<Columns cols={2}>
  ![The view all logs button](/getting-started/sample-projects/multi-agent-view-all-logs-button.webp)
</Columns>

The Log stream is configured to evaluate the following metrics:

- [Action Advancement](/concepts/metrics/agentic/action-advancement)
- [Action Completion](/concepts/metrics/agentic/action-completion)
- [Tool Errors](/concepts/metrics/agentic/tool-error)
- [Tool Selection Quality](/concepts/metrics/agentic/tool-selection-quality)

For some of the traces, these metrics are evaluated at 100%, showing the agents are working well for those inputs. For other traces, these metrics are reporting lower values, showing the chatbot needs some improvements.

![A set of traces with Correctness and Instruction Adherence metrics with a range of values from 33% to 100%](/getting-started/sample-projects/multi-agent-traces.webp)

Select different rows to see more details, including the input and output data, the metric scores, and explanations

### Get insights

Galileo has an Insights Engine that continuously reviews your traces and metrics, and gives suggestions to improve your application. Navigate back to the project page by selecting the **Multi-Agent Banking Chatbot** from the top navigation bar.

Insights will be showing on the right-hand side:

![A list of insights](/getting-started/sample-projects/multi-agent-insights.webp)

Review the generated insights, and think about ways to improve the chatbot by tweaking the agent prompts. The insights will likely have something like this:

> **Summary**
>
> The supervisor agent exhibits inconsistent behavior that undermines the multi-agent system's effectiveness. In a credit score inquiry, the supervisor correctly identified the query type and transferred it to the credit-score-agent, which successfully retrieved the user's credit score (550) and provided helpful context about the score's meaning. However, when control returned to the supervisor, it responded with 'I don't know' despite the specialist having successfully completed the task. This creates a frustrating user experience where the system retrieves the requested information but then claims ignorance, potentially making users think the system is broken or unreliable.
>
> **Suggestions**
>
> Ensure the supervisor agent properly processes and relays the results from specialist agents instead of defaulting to 'I don't know' responses.

To see how you can use these insights to improve the app, get the code and try some different agent prompts.

## Run the sample app

You can run the sample app to generate more traces, and test out different agent prompts.

### Prerequisites

To run the code yourself to generate more traces, you will need:

- Access to an OpenAI compatible API, such as
  - An OpenAI API key
  - Access to an OpenAI compatible API, such as Google Vertex
  - Ollama installed locally with a model downloaded
- A [Pinecone account](https://www.pinecone.io). The free Starter tier is more than enough for this project. You will need your Pinecone API key.
- Either Python 3.9 or later, or Node installed

To get metrics calculated in Galileo, you will need:

- An integration with an LLM configured. If you don't have an integration configured, then:

    <Steps>
    <Step title="Open the integrations page">
    Select the menu in the top right, and select **Integrations**

    ![The integrations menu item](/getting-started/sample-projects/integrations.webp)
    </Step>
    <Step title="Add an integration">
    Select **+ Add Integration** for the LLM you are using and add the relevant details, such as an API key or endpoint.

    ![The integrations menu item](/getting-started/sample-projects/integrations-list.webp)
    </Step>
    </Steps>

### Get the code

<Steps>
<Step title="Clone the SDK examples repo">

```bash Terminal
git clone https://github.com/rungalileo/sdk-examples
```

</Step>

<Step title="Navigate to the relevant project folder">

Start by navigating to the root folder for the programming language you are using:

<CodeGroup>

```bash Python
cd python/agent/langgraph-fsi-agent/after
```

```bash TypeScript
cd typescript/agent/langgraph-fsi-agent
```

</CodeGroup>

<Note>
The Python code for the sample is in a folder called `after`. If you want to learn more about adding logging with Galileo to a LangGraph app, check out the [add evaluations to a multi-agent LangGraph application cookbook](/cookbooks/use-cases/multi-agent-langgraph/multi-agent-langgraph).
</Note>

</Step>
</Steps>

The full source code for all of our sample projects is available in the Galileo [SDK Examples GitHub repo](https://github.com/rungalileo/sdk-examples).

<CardGroup cols={2}>
<Card title="SDK Examples" icon="code" horizontal href="https://github.com/rungalileo/sdk-examples">
    Check out sample projects using Galileo
</Card>
</CardGroup>

### Set up Pinecone

This project uses Pinecone as a vector database to power a RAG agent that retrieves data around the fictional credit cards offered by a bank. Before you can run the app, you will need to upload the documents.

<Steps>

<Step title="Configure environment variables">
In each project folder is a `.env.example` file. Rename this file to `.env` and populate the `PINECONE_API_KEY` value. You can leave the other values for now as you will populate them later
</Step>
<Step title="Upload the documents">
There is a helper script in the `scripts` folder. Run this script to create a new index in Pinecone and upload the documents.

<CodeGroup>

```python Python
python ./scripts/setup_pinecone.py
```

```typescript TypeScript
npm run upload-docs
```

</CodeGroup>

This will take a few seconds, and a successful run should look like:

```output Terminal
Loading documents for credit-card-information folder...
...
âœ… Document processing and upload complete!
```

</Step>
</Steps>

### Run the code

<Steps>
<Step title="Install required dependencies">

From the project folder, Install the required dependencies. For Python, make sure to create and activate a virtual environment before installing the dependencies.

<CodeGroup>

```bash Python
pip install -r requirements.txt
```

```bash TypeScript
npm install
```

</CodeGroup>

</Step>

<Step title="Configure environment variables">
In your `.env` file, populate the Galileo values:

| Environment Variable  | Value |
| :-------------------- | :---- |
| `GALILEO_API_KEY`     | Your API key |
| `GALILEO_PROJECT`     | The name of your Galileo project - this is preset to `Multi-Agent Banking Chatbot` |
| `GALILEO_LOG_STREAM`  | The name of your Log stream - this is preset to `Default Log Stream` |
| `GALILEO_CONSOLE_URL` | Optional. The URL of your Galileo console for custom deployments. For the fre tier, you don't need to set this. |

<Note>
You can find these values from the project page for the multi-agent banking chatbot sample page in the Galileo Console.
</Note>

Next populate the values for your LLM:

| Environment Variable | Value |
| :------------------- | :---- |
| `OPENAI_API_KEY`     | Your OpenAI API key. If you are using Ollama, set this to `ollama`. If you are using another OpenAI compatible API, then set this to the relevant API key. |
| `OPENAI_BASE_URL`    | Optional. The base URL of your OpenAI deployment. Leave this commented out if you are using the default OpenAI API. If you are using Ollama, set this to `http://localhost:11434/v1`. If you are using another OpenAI compatible API, then set this to the relevant URL. |
| `MODEL_NAME`         | The name of the model you are using |

</Step>

<Step title="Run the project">
Run the project with the following command:

<CodeGroup>

```bash Python
chainlit run app.py -w
```

```bash TypeScript
npm run start
```

</CodeGroup>

If you are using the Python version, the app will be running at [localhost:8000](http://localhost:8000), so open it in your browser.

![A demo of the bot responding to being asked what credit cards do you offer. The bot lists 2 cards](/cookbooks/use-cases/multi-agent-langgraph/bot-demo.gif)

If you are using TypeScript, the app will run in your terminal:

<CodeGroup>

```output Terminal
You: What credit cards do you offer?
Assistant: Brahe Bank offers the Orbit Basic Credit Card, which features no
  annual fee, variable interest rates (29.9% APR on purchases, 34.9% APR
  on cash advances), and a 0% APR on balance transfers for 12 months. It
  does not include a rewards program but offers standard fraud protection,
  digital card management tools, and contactless payment support.

  Eligibility requires being 18 or older with a valid U.S. address and SSN,
  and a fair credit history with a credit score over 500.
  Would you like information on eligibility or application process?
```

</CodeGroup>

You can ask the agent questions about:

- The different credit cards offered by the bank
- Your credit score

</Step>
</Steps>

### Improve the app

The insights you viewed earlier suggested improving how the supervisor agent processes messages, especially with credit scores. You can try this out to see what issues might occur:

```output Terminal
You: What is my credit score?
Assistant: I cannot answer that question.
```

Despite there being an agent to get the users credit score, it is not always used.

To improve the agent, have a look at the agent prompt defined in the following file:

<CodeGroup>

```output Python
src/galileo_langgraph_fsi_agent/agents/supervisor_agent.py
```

```output TypeScript
agents/supervisorAgent.ts
```

</CodeGroup>

In this file is the current agent prompt:

<CodeGroup>

```python Python
bank_supervisor_agent = create_supervisor(
    model=ChatOpenAI(model=os.environ["MODEL_NAME"], name="Supervisor"),
    agents=[credit_card_information_agent, credit_score_agent],
    prompt=(
        """
        You are a supervisor managing the following agents:
        - a credit card information agent. Assign any tasks related to
          information about credit cards to this agent
        Otherwise, only respond with 'I don't know' or 'I cannot answer
        that question'.
        If you need to ask the user for more information, do so in a
        concise manner.
        """
    ),
    add_handoff_back_messages=True,
    output_mode="full_history",
    supervisor_name="brahe-bank-supervisor-agent",
).compile()
```

```typescript TypeScript
const bankSupervisorAgent = createSupervisor({
    llm: new ChatOpenAI({ model: process.env.MODEL_NAME }),
    agents: [creditCardInformationAgent, creditScoreAgent],
    prompt: `
        You are a supervisor managing the following agents:
        - a credit card information agent. Assign any tasks related to
          information about credit cards to this agent
        Otherwise, only respond with 'I don't know' or 'I cannot answer
        that question'.
        If you need to ask the user for more information, do so in a
        concise manner.
    `,
    addHandoffBackMessages: true,
    outputMode: "full_history",
    supervisorName: "brahe-bank-supervisor-agent",
}).compile();
```

</CodeGroup>

This supervisor agent prompt explicitly mentions the credit card agent, but not the credit score agent. You can encourage the supervisor agent to use the credit score agent to get better results:

<CodeGroup>

```python Python
"""
You are a supervisor managing the following agents:
- a credit card information agent. Assign any tasks related to
    information about credit cards to this agent
- a credit score agent. Use this to get the users credit score.
Otherwise, only respond with 'I don't know' or 'I cannot answer
that question'.
If you need to ask the user for more information, do so in a
concise manner.
"""
```

```typescript TypeScript
`
You are a supervisor managing the following agents:
- a credit card information agent. Assign any tasks related to
    information about credit cards to this agent
- a credit score agent. Use this to get the users credit score.
Otherwise, only respond with 'I don't know' or 'I cannot answer
that question'.
If you need to ask the user for more information, do so in a
concise manner.
`
```

</CodeGroup>

Try this new prompt out and see how the agent responds.

```output Terminal
You: what is my credit score
Assistant: Your credit score is 550. If you have any other questions or
need further assistance, please let me know.
```

Once you have asked a few questions, head back to the Galileo Console and examine the new traces. You should see the metrics improving.

### Run the sample app as an experiment

Galileo allows you to run [experiments](/concepts/experiments/overview) against [datasets](/concepts/datasets) of known data, generating traces in an experiment Log stream and evaluating these for different metrics. Experiments allow you to take a known set of inputs and evaluate different prompts, LLMs, or versions of your apps.

This sample project has a unit test that runs the chatbot against a pre-defined dataset, containing a mixture of sensible and irrelevant questions:

```json dataset.json
[
    {"input": "What are the cashback rewards offered by the Orbit Credit Card?"},
    {"input": "What is my credit score?"},
    {"input": "What is the APR for balance transfers on the Orbit Credit Card?"},
    {"input": "What credit cards am I eligible for?"},
    {"input": "What can I do with my credit score?"},
    {"input": "Recommend me a good book."}
    ...
]
```

You can use this unit test to evaluate different supervisor agent prompts for your app.

<Steps>

<Step title="Run the unit test">

Use the following command to run the unit test:
<CodeGroup>

```python Python
python -m pytest test.py
```

```typescript TypeScript
npm run test
```

</CodeGroup>
</Step>

<Step title="Evaluate the experiment">
The unit test will output a link to the experiment in the Galileo Console:

```output Terminal
Experiment multi-agent-chatbot-experiment 2025-07-15 at 00:48:11.842 has
completed and results are available at
https://app.galileo.ai/project/<id>/experiments/<id>
```

Follow this link to see the metrics for the experiment Log stream.

![The experiment with low correctness scores for most rows](/getting-started/sample-projects/multi-agent-experiment.webp)

</Step>

<Step title="Try different supervisor agent prompts">

Experiment with different supervisor agent prompts. Edit the supervisor agent prompt in the app, then re-run the experiment through the unit test to see how different supervisor agent prompts affect the metrics.

</Step>
<Step title="Compare experiments">
If you navigate to the experiments list using the **All Experiments** link, you will be able to compare the average metric values of each run.

![A list of experiments with the scores increasing as you go up the list](/getting-started/sample-projects/compare-experiments.webp)

You can then select multiple rows and [compare the experiments in detail](/concepts/experiments/compare).
</Step>
</Steps>

## Next steps

### Logging with the SDKs

<CardGroup cols={2}>
<Card title="Learn how to log experiments" icon="flask" horizontal href="/sdk-api/experiments">
    Learn how to run experiments with multiple data points using datasets and prompt templates
</Card>
<Card title="Galileo logger" icon="code" horizontal href="/sdk-api/logging/galileo-logger">
    Log with full control over sessions, traces, and spans using the Galileo logger.
</Card>
<Card title="Log decorator" icon="code" horizontal href="/sdk-api/logging/log-decorator/log-decorator">
    Quickly add logging to your code with the log decorator and wrapper.
</Card>
<Card title="Galileo context" icon="code" horizontal href="/sdk-api/logging/galileo-context">
    Manage logging using the Galileo context manager.
</Card>
</CardGroup>

### How-to guides

<CardGroup cols={2}>
<Card title="Log Using the OpenAI Wrapper" icon="code" href="/how-to-guides/basics/basic-example" horizontal>
  Learn how to integrate and use OpenAI's API with Galileo's wrapper client.
  <br/>
  **Python**
</Card>

<Card title="Log Using the @log Decorator" icon="code" href="/how-to-guides/basics/basic-logging-with-decorator/basic-logging-with-decorator" horizontal>
  Learn how to use the Galileo @log decorator to log functions to traces
  <br/>
  **Python**
</Card>

<Card title="Create Traces and Spans" icon="code" href="/how-to-guides/basics/manual-span-creation/manual-span-creation" horizontal>
  Learn how to create log traces and spans manually in your AI apps
  <br/>
  **Python**
</Card>
</CardGroup>

### SDK reference

<CardGroup cols={2}>
<Card title="Python SDK Reference" icon="python" horizontal href="/sdk-api/python/sdk-reference">
    The Galileo Python SDK reference.
</Card>
<Card title="TypeScript SDK Reference" icon="js" horizontal href="/sdk-api/typescript/sdk-reference">
    The Galileo TypeScript SDK reference.
</Card>
</CardGroup>
