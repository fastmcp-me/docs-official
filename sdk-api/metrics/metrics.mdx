---
title: Metrics
description: A quick lookup for integrating and interpreting metrics in your workflows.
syncTabs: true
---

import MetricsReferenceTable from "/snippets/content/metrics-reference-table.mdx"

import SnippetExperimentsPresetMetricsPython from "/snippets/code/python/concepts/metrics/preset-metrics.mdx";
import SnippetExperimentsCustomMetricsPython from "/snippets/code/python/concepts/metrics/custom-metrics.mdx";

import SnippetExperimentsPresetMetricsTypeScript from "/snippets/code/typescript/concepts/metrics/preset-metrics.mdx";
import SnippetExperimentsCustomMetricsTypeScript from "/snippets/code/typescript/concepts/metrics/custom-metrics.mdx";

Galileo AI provides a set of built-in, preset metrics designed to evaluate various aspects of LLM, agent, and retrieval-based workflows. This guide provides a reference for Galileo AI's preset metrics, including and SDK slugs.

<MetricsReferenceTable />

## How do I use metrics in the SDK?

The `run experiment` function ([Python](/sdk-api/python/reference#galileo-experiments), [TypeScript](/sdk-api/typescript/reference#galileo-experiments)) takes a list of metrics as part of its arguments. 

### Preset metrics

Supply a list of one or more metric names into the `run_experiment` function as shown below:

<CodeGroup>
  <SnippetExperimentsPresetMetricsPython/>
  <SnippetExperimentsPresetMetricsTypeScript/>
</CodeGroup>

For more information, read about running experiments with the [Galileo SDKs](/sdk-api/experiments).

### Custom metrics

You can use [custom metrics](/concepts/metrics/custom-metrics/custom-metrics-ui-code) in the same way as Galileo's preset metrics. At a high level, this involves the following steps:

1. Create your metric in the [Galileo console](https://app.galileo.ai) (or in [code](/concepts/metrics/custom-metrics/custom-metrics-ui-code#custom-scorers)). Your custom metric will return a numerical score based on its input.
1. Pass the name of your new metric into the `run experiment`, like in the example below. 

In this example, we reference a custom metric that was saved in the console with the name `My custom metric`.

![A custom metric in the console with the description "counts the length of the input and output"](/images/g2/custom-metric-name-example.png)

<CodeGroup>
  <SnippetExperimentsCustomMetricsPython/>
  <SnippetExperimentsCustomMetricsTypeScript/>
</CodeGroup>

Custom metrics provide the flexibility to define precisely what you want to measure, enabling deep analysis and targeted improvement. For a detailed walkthrough on creating them, see [Custom Metrics](/concepts/metrics/custom-metrics/custom-metrics-ui-code).

## Ground truth data

**Ground truth** is the authoritative, validated answer or label used to benchmark model performance. For LLM metrics, this often means a gold-standard answer, fact, or supporting evidence against which outputs are compared.

The following metrics require ground truth data to compute their scores, as they involve direct comparison to a reference answer, label, or fact.

- [BLEU and ROUGE](/concepts/metrics/expression-and-readability/bleu-and-rouge)
- [Ground Truth Adherence](/concepts/metrics/response-quality/ground-truth-adherence)

<Note>
These metrics are only supported in experiments, as they require the ground truth to be set in the dataset used by the experiment.
</Note>

To set the ground truth, set this in the output of your dataset either in the [Galileo console](/concepts/datasets#create-and-manage-datasets-via-the-console), or [in code](/concepts/experiments/running-experiments#ground-truth).

## Are metrics LLM-agnostic?

Yes, all metrics are designed to work across any LLM integrated with Galileo.

## References

- [Metrics Overview](/concepts/metrics/overview)
- [Experiments Overview](/concepts/experiments/overview)
- [Run Experiments in code](/concepts/experiments/running-experiments)
- [Use Custom Metrics](/concepts/experiments/running-experiments#custom-metrics-for-deep-analysis)
