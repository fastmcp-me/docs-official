---
title: Metrics Basics
description: Learn how to use metrics in code
---

{/*<!-- markdownlint-disable MD044 -->*/}

import MetricsReferenceTable from "/snippets/content/metrics-reference-table.mdx"

import SnippetPresetMetricsPython from "/snippets/code/python/concepts/metrics/preset-metrics.mdx";
import SnippetExperimentCustomMetricNamePython from "/snippets/code/python/sdk/experiments/custom-metric.mdx";

import SnippetPresetMetricsTypeScript from "/snippets/code/typescript/concepts/metrics/preset-metrics.mdx";
import SnippetExperimentCustomMetricNameTypeScript from "/snippets/code/typescript/sdk/experiments/custom-metric.mdx";

{/*<!-- markdownlint-enable MD044 -->*/}

Galileo AI provides a set of built-in, preset metrics designed to evaluate various aspects of LLM, agent, and retrieval-based workflows. This guide provides a reference for Galileo AI's preset metrics, including and SDK slugs.

<MetricsReferenceTable />

## How do I use metrics in the SDK?

The `run experiment` function ([Python](/sdk-api/python/reference#galileo-experiments), [TypeScript](/sdk-api/typescript/reference#galileo-experiments)) takes a list of metrics as part of its arguments.

### Preset metrics

Supply a list of one or more metric names into the `run_experiment` function as shown below:

<CodeGroup>
  <SnippetPresetMetricsPython/>
  <SnippetPresetMetricsTypeScript/>
</CodeGroup>

For more information, read about running experiments with the [Galileo SDKs](/sdk-api/experiments).

### Custom metrics

You can use [custom metrics](/concepts/metrics/custom-metrics/custom-metrics-ui-code) in the same way as Galileo's preset metrics. At a high level, this involves the following steps:

1. Create your metric in the [Galileo Console](https://app.galileo.ai) (or in [code](/concepts/metrics/custom-metrics/custom-metrics-ui-code#custom-scorers)). Your custom metric will return a numerical score based on its input.
1. Pass the name of your new metric into the `run experiment`, like in the example below.

For example, if you have a metric called `"Compliance - do not recommend any financial actions"`:

![A metric called Compliance - do not recommend any financial actions](/concepts/metrics/custom-metrics/metric-name.webp)

You would pass this to an experiment like this:

<CodeGroup>
<SnippetExperimentCustomMetricNamePython/>
<SnippetExperimentCustomMetricNameTypeScript/>
</CodeGroup>

Custom metrics provide the flexibility to define precisely what you want to measure, enabling deep analysis and targeted improvement. For a detailed walkthrough on creating them, see [Custom Metrics](/concepts/metrics/custom-metrics/custom-metrics-ui-code).

## Ground truth data

**Ground truth** is the authoritative, validated answer or label used to benchmark model performance. For LLM metrics, this often means a gold-standard answer, fact, or supporting evidence against which outputs are compared.

The following metrics require ground truth data to compute their scores, as they involve direct comparison to a reference answer, label, or fact.

- [BLEU and ROUGE](/concepts/metrics/expression-and-readability/bleu-and-rouge)
- [Ground Truth Adherence](/concepts/metrics/response-quality/ground-truth-adherence)

<Note>
These metrics are only supported in experiments, as they require the ground truth to be set in the dataset used by the experiment.
</Note>

To set the ground truth, set this in the output of your dataset either in the [Galileo Console](/concepts/datasets#create-and-manage-datasets-via-the-console), or [in code](/concepts/experiments/running-experiments#ground-truth).

## Are metrics LLM-agnostic?

Yes, all metrics are designed to work across any LLM integrated with Galileo.

## References

- [Metrics Overview](/concepts/metrics/overview)
- [Experiments Overview](/concepts/experiments/overview)
- [Run Experiments in code](/concepts/experiments/running-experiments)
- [Use Custom Metrics](/concepts/experiments/running-experiments#custom-metrics-for-deep-analysis)
