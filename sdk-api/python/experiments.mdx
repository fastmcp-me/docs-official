---
title: Experiments
description: Run experiments with multiple data points
---

import SnippetExperimentsPrompt from "/snippets/code/python/concepts/experiments/prompt.mdx";
import SnippetExperimentsExistingDataset from "/snippets/code/python/concepts/experiments/existing-dataset.mdx";
import SnippetExperimentsCustomDataset from "/snippets/code/python/concepts/experiments/custom-dataset.mdx";
import SnippetExperimentsCustomMetrics from "/snippets/code/python/concepts/experiments/custom-metrics.mdx";

Experiments in Galileo allow you to evaluate and compare different prompts, models, and configurations using datasets, and measure their performance using various metrics. This helps you identify the best approach for your specific use case.

For a list of supported metrics, see the [Metrics Reference Guide](/concepts/metrics/metrics-reference).


## Run an Experiment with a Prompt Template

The simplest way to get started is by using a prompt template. 

* If you have an existing prompt template, you can fetch it by importing and using the [`get_prompt_template`](/sdk-api/python/prompts#getting-prompt-templates) function from `galileo.prompts`.
* The `get_dataset` function below expects a dataset that you created through either the [console](https://app.galileo.ai/datasets) or the [SDK](/sdk-api/python/datasets#creating-datasets). Ensure you have saved a dataset before running the experiment!

<CodeGroup>
  <SnippetExperimentsPrompt />
</CodeGroup>

## Run Experiments with Custom Functions

For more complex scenarios, you can use custom functions with the OpenAI wrapper. Here, you may use either a [saved dataset](/sdk-api/python/datasets#creating-datasets) or a [custom one](#custom-dataset-evaluation)

<CodeGroup>
  <SnippetExperimentsExistingDataset />
</CodeGroup>

## Custom Dataset Evaluation

When you need to test specific scenarios:

<CodeGroup>
  <SnippetExperimentsCustomDataset />
</CodeGroup>

## Custom Metrics for Deep Analysis

For sophisticated evaluation needs, you can also create metrics in code and use them in experiments. For a detailed walkthrough of the code sample below, take a look at [Local Metrics](/concepts/metrics/custom-metrics/calling-local-scorers-python). 

<CodeGroup>
  <SnippetExperimentsCustomMetrics />
</CodeGroup>

## Best Practices

1. **Use consistent datasets**: Use the same dataset when comparing different prompts or models to ensure fair comparisons.

2. **Test multiple variations**: Run experiments with different prompt variations to find the best approach.

3. **Use appropriate metrics**: Choose metrics that are relevant to your specific use case.

4. **Start small**: Begin with a small dataset to quickly iterate and refine your approach before scaling up.

5. **Document your experiments**: Keep track of what you're testing and why to make it easier to interpret results.

## Related Resources

### Concepts 

- [What are Datasets?](/concepts/datasets) - Learn about Datasets and how to work with them

### Guides

- [Creating Datasets](/sdk-api/python/experimentation/datasets) - Creating and managing datasets for experiments in Python
- [Creating Prompt Templates](/sdk-api/python/prompts) - Creating and using prompt templates in Python
- Work with Custom Metrics:
  - [Local Metrics](/concepts/metrics/custom-metrics/calling-local-scorers-python) - Create and run custom metrics directly in code
  - [Code-based scoring](/concepts/metrics/custom-metrics/custom-metrics-ui-code) - Create reusable custom metrics right in the Galileo Console.
  - [LLM-as-judge](/concepts/metrics/custom-metrics/custom-metrics-ui-llm) - Create reusable custom metrics using LLMs to evaluate your response quality

### References

- [Metrics Reference Guide](/concepts/metrics/metrics-reference) - A list of supported metrics and how to use them in experiments
