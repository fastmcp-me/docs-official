---
title: experiments
---

{/* This page is autogenerated from the Python SDK. Do not edit this file as it will be replaced on the next SDK publish */}


#### run\_experiment

```python
def run_experiment(experiment_name: str,
                   *,
                   prompt_template: Optional[PromptTemplate] = None,
                   prompt_settings: Optional[PromptRunSettings] = None,
                   project: Optional[str] = None,
                   project_id: Optional[str] = None,
                   dataset: Optional[Union[Dataset, list[Union[dict[str, Any],
                                                               str]],
                                           str]] = None,
                   dataset_id: Optional[str] = None,
                   dataset_name: Optional[str] = None,
                   metrics: Optional[list[Union[GalileoScorers, Metric,
                                                LocalMetricConfig,
                                                str]]] = None,
                   function: Optional[Callable] = None) -> Any
```

Run an experiment with the specified parameters.

There are two ways to run an experiment:
1. Using a prompt template, prompt settings, and a dataset
2. Using a runner function and a dataset

When using a runner function, you can also pass a list of dictionaries to the function to act as a dataset.

The project can be specified by providing exactly one of the project name (via the &#x27;project&#x27; parameter or the GALILEO_PROJECT environment variable) or the project ID (via the &#x27;project_id&#x27; parameter or the GALILEO_PROJECT_ID environment variable).

**Arguments**:

- `experiment_name`: Name of the experiment
- `prompt_template`: Template for prompts
- `prompt_settings`: Settings for prompt runs
- `project`: Optional project name. Takes preference over the GALILEO_PROJECT environment variable. Leave empty if using project_id
- `project_id`: Optional project Id. Takes preference over the GALILEO_PROJECT_ID environment variable. Leave empty if using project
- `dataset`: Dataset object, list of records, or dataset name
- `dataset_id`: ID of the dataset
- `dataset_name`: Name of the dataset
- `metrics`: List of metrics to evaluate
- `function`: Optional function to run with the experiment

**Raises**:

- `prompt_template`0: If required parameters are missing or invalid

**Returns**:

Experiment run results

#### get\_experiment

```python
def get_experiment(
    project_id: str, experiment_name: str
) -> Optional[Union[ExperimentResponse, HTTPValidationError]]
```

Get an experiment with the specified parameters.

**Arguments**:

- `project_id`: Galileo ID of the project associated with this experiment
- `experiment_name`: Name of the experiment

**Raises**:

- `HTTPValidationError`: If there&#x27;s a validation error in returning a ExperimentResponse

**Returns**:

ExperimentResponse results

#### get\_experiments

```python
def get_experiments(
    project_id: str
) -> Optional[Union[HTTPValidationError, list[ExperimentResponse]]]
```

Get an experiments with the specified Project ID.

**Arguments**:

- `project_id`: Galileo ID of the project associated with this experiment

**Raises**:

- `HTTPValidationError`: If there&#x27;s a validation error in returning a list of ExperimentResponse

**Returns**:

List of ExperimentResponse results

