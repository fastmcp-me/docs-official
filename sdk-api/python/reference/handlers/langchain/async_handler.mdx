---
title: async_handler
---

{/* This page is autogenerated from the Python SDK. Do not edit this file as it will be replaced on the next SDK publish */}

## GalileoAsyncCallback

Async Langchain callback handler for logging traces to the Galileo platform.

**Arguments**

- `_handler` (`GalileoAsyncBaseHandler`): The async handler for managing the trace.

### on_agent_finish

```python
async def on_agent_finish(self,
                          finish: AgentFinish,
                          *,
                          run_id: UUID,
                          **kwargs: Any) -> Any
```

Langchain callback when an agent finishes.

### on_chain_end

```python
async def on_chain_end(self,
                       outputs: dict[str, Any],
                       *,
                       run_id: UUID,
                       parent_run_id: Optional[UUID]=None,
                       **kwargs: Any) -> Any
```

Langchain callback when a chain ends.

### on_chain_error

```python
async def on_chain_error(self,
                         error: Exception,
                         *,
                         run_id: UUID,
                         parent_run_id: Optional[UUID]=None,
                         **kwargs: Any) -> Any
```

Called when a chain errors.

### on_chain_start

```python
async def on_chain_start(self,
                         serialized: dict[str, Any],
                         inputs: dict[str, Any],
                         *,
                         run_id: UUID,
                         parent_run_id: Optional[UUID]=None,
                         tags: Optional[list[str]]=None,
                         **kwargs: Any) -> Any
```

Langchain callback when a chain starts.

### on_chat_model_start

```python
async def on_chat_model_start(self,
                              serialized: dict[str, Any],
                              messages: list[list[BaseMessage]],
                              *,
                              run_id: UUID,
                              parent_run_id: Optional[UUID]=None,
                              tags: Optional[list[str]]=None,
                              metadata: Optional[dict[str, Any]]=None,
                              **kwargs: Any) -> Any
```

Langchain callback when a chat model starts.

### on_llm_end

```python
async def on_llm_end(self,
                     response: LLMResult,
                     *,
                     run_id: UUID,
                     parent_run_id: Optional[UUID]=None,
                     **kwargs: Any) -> Any
```

Langchain callback when an LLM node ends.

### on_llm_error

```python
async def on_llm_error(self,
                       error: Exception,
                       *,
                       run_id: UUID,
                       parent_run_id: Optional[UUID]=None,
                       **kwargs: Any) -> Any
```

Called when an LLM errors.

### on_llm_new_token

```python
async def on_llm_new_token(self, token: str, *, run_id: UUID, **kwargs: Any) -> Any
```

Langchain callback when an LLM node generates a new token.

### on_llm_start

```python
async def on_llm_start(self,
                       serialized: dict[str, Any],
                       prompts: list[str],
                       *,
                       run_id: UUID,
                       parent_run_id: Optional[UUID]=None,
                       tags: Optional[list[str]]=None,
                       metadata: Optional[dict[str, Any]]=None,
                       **kwargs: Any) -> Any
```

Langchain callback when an LLM node starts.

Note: This callback is only used for non-chat models.

### on_retriever_end

```python
async def on_retriever_end(self,
                           documents: list[Document],
                           *,
                           run_id: UUID,
                           parent_run_id: Optional[UUID]=None,
                           **kwargs: Any) -> Any
```

Langchain callback when a retriever node ends.

### on_retriever_error

```python
async def on_retriever_error(self,
                             error: Exception,
                             *,
                             run_id: UUID,
                             parent_run_id: Optional[UUID]=None,
                             **kwargs: Any) -> Any
```

Called when a retriever errors.

### on_retriever_start

```python
async def on_retriever_start(self,
                             serialized: dict[str, Any],
                             query: str,
                             *,
                             run_id: UUID,
                             parent_run_id: Optional[UUID]=None,
                             tags: Optional[list[str]]=None,
                             metadata: Optional[dict[str, Any]]=None,
                             **kwargs: Any) -> Any
```

Langchain callback when a retriever node starts.

### on_tool_end

```python
async def on_tool_end(self,
                      output: Any,
                      *,
                      run_id: UUID,
                      parent_run_id: Optional[UUID]=None,
                      **kwargs: Any) -> Any
```

Langchain callback when a tool node ends.

### on_tool_error

```python
async def on_tool_error(self,
                        error: Exception,
                        *,
                        run_id: UUID,
                        parent_run_id: Optional[UUID]=None,
                        **kwargs: Any) -> Any
```

Called when a tool errors.

### on_tool_start

```python
async def on_tool_start(self,
                        serialized: dict[str, Any],
                        input_str: str,
                        *,
                        run_id: UUID,
                        parent_run_id: Optional[UUID]=None,
                        tags: Optional[list[str]]=None,
                        metadata: Optional[dict[str, Any]]=None,
                        **kwargs: Any) -> Any
```

Langchain callback when a tool node starts.
