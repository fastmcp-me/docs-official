---
title: LangChain Integration
---

import SnippetLangChainInstallation from "/snippets/code/python/sdk/wrappers/langchain-installation.mdx";
import SnippetLangChainEnvSetup from "/snippets/code/python/sdk/wrappers/langchain-env-setup.mdx";
import SnippetLangChainBasic from "/snippets/code/python/sdk/wrappers/langchain.mdx";
import SnippetLangChainCustomLogger from "/snippets/code/python/sdk/wrappers/langchain-custom-logger.mdx";
import SnippetLangChainChains from "/snippets/code/python/sdk/wrappers/langchain-chains.mdx";
import SnippetLangChainAgents from "/snippets/code/python/sdk/wrappers/langchain-agents.mdx";
import SnippetLangChainMetadata from "/snippets/code/python/sdk/wrappers/langchain-metadata.mdx";
import SnippetLangChainContext from "/snippets/code/python/sdk/wrappers/langchain-context.mdx";

The Galileo LangChain integration allows you to automatically log all LangChain interactions with LLMs, including prompts, responses, and performance metrics. This integration works through LangChain's callbacks API, making it easy to add logging to your existing LangChain applications with minimal code changes.

## Installation

First, make sure you have the Galileo SDK and LangChain installed:

<CodeGroup>
  <SnippetLangChainInstallation />
</CodeGroup>

## Setup

Create or update a `.env` file with your Galileo API key and other optional settings:

<CodeGroup>
  <SnippetLangChainEnvSetup />
</CodeGroup>

## Basic Usage

The integration is based on the `GalileoCallback` class, which implements LangChain's callback interface. To use it, simply create an instance of the callback and pass it to your LangChain components:

<CodeGroup>
  <SnippetLangChainBasic />
</CodeGroup>

When initializing the `GalileoCallback`, you can optionally specify a Galileo logger instance or control trace behavior:

<CodeGroup>
  <SnippetLangChainCustomLogger />
</CodeGroup>

## Using with LangChain Chains

You can also use the callback with LangChain chains. Make sure to pass the callback to both the LLM and the chain:

<CodeGroup>
  <SnippetLangChainChains />
</CodeGroup>

## Advanced Usage

The `GalileoCallback` captures various LangChain events, including:

- LLM starts and completions
- Chat model interactions
- Chain executions
- Tool calls
- Retriever operations
- Agent actions

For each of these events, the callback logs relevant information to Galileo, such as:

- Input prompts and messages
- Output responses
- Model information
- Timing data
- Token usage
- Error information (if any)

### Adding Metadata

You can add custom metadata to your logs by including it in the `metadata` parameter when invoking a chain or LLM:

<CodeGroup>
  <SnippetLangChainMetadata />
</CodeGroup>

This metadata will be attached to the logs in Galileo, making it easier to filter and analyze your data.

### Nested Chains and Agents

The `GalileoCallback` automatically handles nested chains and agents, creating a hierarchical trace that reflects the structure of your LangChain application:

<CodeGroup>
  <SnippetLangChainAgents />
</CodeGroup>

## Best Practices

1. **Pass callbacks consistently**: Make sure to pass the callback to all LangChain components (LLMs, chains, agents, etc.) to ensure complete logging.

2. **Include meaningful metadata**: Add relevant metadata to your invocations to make it easier to filter and analyze your logs.

3. **Use with `galileo_context`**: You can combine the LangChain integration with `galileo_context` for more control over trace management:

<CodeGroup>
  <SnippetLangChainContext />
</CodeGroup>

## Related Resources

- [OpenAI Wrapper](/sdk-api/python/wrappers/openai) - For automatic logging of OpenAI calls
- [`@log` Decorator](/sdk-api/python/logging/log-decorator) - For decorating functions with logging
- [`galileo_context`](/sdk-api/python/logging/galileo-context) - For managing trace context and automatic flushing
