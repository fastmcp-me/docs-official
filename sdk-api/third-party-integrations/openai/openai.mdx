---
title: OpenAI SDK
description: Learn about the Galileo OpenAI integration
---

{/*<!-- markdownlint-disable MD044 -->*/}

import SnippetSdkWrappersInstallationPython from "/snippets/code/python/sdk/installation-openai.mdx";
import SnippetSdkWrappersOpenAIBasicPython from "/snippets/code/python/sdk/wrappers/openai-basic.mdx";
import SnippetSdkWrappersOpenAIAutoSessionsPython from "/snippets/code/python/sdk/wrappers/openai-auto-sessions.mdx";
import SnippetSdkWrappersOpenAIManualSessionPython from "/snippets/code/python/sdk/wrappers/openai-manual-session.mdx";
import SnippetSdkWrappersOpenAIManualTracePython from "/snippets/code/python/sdk/wrappers/openai-manual-trace.mdx";
import SnippetSdkWrappersOpenAIStreamingPython from "/snippets/code/python/sdk/wrappers/openai-streaming.mdx";
import SnippetSdkWrappersOpenAILogDecoratorPython from "/snippets/code/python/sdk/wrappers/openai-log-decorator.mdx";

import SnippetNpmInstall from "/snippets/code/typescript/sdk/installation/npm.mdx";
import SnippetYarnInstall from "/snippets/code/typescript/sdk/installation/yarn.mdx";
import SnippetPnpmInstall from "/snippets/code/typescript/sdk/installation/pnpm.mdx";
import SnippetOpenAIUsageTypeScript from "/snippets/code/typescript/sdk/wrappers/openai-usage.mdx";
import SnippetSdkWrappersOpenAIAutoSessionsTypeScript from "/snippets/code/typescript/sdk/wrappers/openai-auto-sessions.mdx";
import SnippetSdkWrappersOpenAIManualSessionTypeScript from "/snippets/code/typescript/sdk/wrappers/openai-manual-session.mdx";
import SnippetSdkWrappersOpenAIManualTraceTypeScript from "/snippets/code/typescript/sdk/wrappers/openai-manual-trace.mdx";
import SnippetSdkWrappersOpenAIStreamingTypeScript from "/snippets/code/typescript/sdk/wrappers/openai-streaming.mdx";
import SnippetSdkWrappersOpenAILogDecoratorTypeScript from "/snippets/code/typescript/sdk/wrappers/openai-log-function.mdx";

{/*<!-- markdownlint-enable MD044 -->*/}

The OpenAI wrapper is the simplest way to integrate Galileo logging into your application. By using Galileo's OpenAI wrapper instead of importing the OpenAI library directly, you can automatically log all prompts, responses, and statistics without any additional code changes in an LLM span for every call.

<Note>
  The Galileo OpenAI wrapper currently only supports the synchronous chat completions API.
</Note>

The wrapper supports both the OpenAI and Azure OpenAI APIs, so will work out of the box with Azure deployments, as well as any LLM that supports the OpenAI SDK.

<CardGroup cols={2}>
<Card title="Python Galileo OpenAI SDK reference" icon="python" horizontal href="/sdk-api/python/reference/openai">
    The Python Galileo OpenAI SDK reference.
</Card>
<Card title="TypeScript Galileo OpenAI SDK reference" icon="js" horizontal href="/sdk-api/typescript/reference/README/functions/wrapOpenAI">
    The TypeScript Galileo OpenAI SDK reference.
</Card>
</CardGroup>

## Installation

First, make sure you have the Galileo SDK installed. If you are using Python, ensure you install the OpenAI optional dependency.

<CodeGroup>
  <SnippetSdkWrappersInstallationPython />
  <SnippetNpmInstall />
  <SnippetYarnInstall />
  <SnippetPnpmInstall />
</CodeGroup>

## Basic usage

If you are using Python, import the `galileo.openai` module, instead of the OpenAI `openai` module and use that to create your client. If you are using TypeScript, use the wrapper to wrap your OpenAI client.

<CodeGroup>
  <SnippetSdkWrappersOpenAIBasicPython />
  <SnippetOpenAIUsageTypeScript />
</CodeGroup>

This example will automatically produce a single-span trace in your Log stream. The wrapper handles all the logging for you, capturing:

- The input prompt
- The model used
- The response
- Timing information
- Token usage
- Other relevant metadata

## Sessions and traces

If you use the OpenAI wrapper by itself, it will automatically create a session and start a new trace for you, adding the call as an LLM span. Subsequent calls will be added as an LLM span to a new trace in the same session. The session will have an autogenerated name based off the content.

<CodeGroup>
  <SnippetSdkWrappersOpenAIAutoSessionsPython />
  <SnippetSdkWrappersOpenAIAutoSessionsTypeScript />
</CodeGroup>

![A single session with 2 traces in the Galileo Console](/sdk-api/third-party-integrations/openai/one-session-two-traces.webp)

If you manually start a session before using the OpenAI wrapper, all calls to the wrapper will be added as new traces to that session.

<CodeGroup>
  <SnippetSdkWrappersOpenAIManualSessionPython />
  <SnippetSdkWrappersOpenAIManualSessionTypeScript />
</CodeGroup>

![A single session called Chat about the Roman Empire with 2 traces in the Galileo Console](/sdk-api/third-party-integrations/openai/manual-session-two-traces.webp)

If you create a new trace before using the OpenAI wrapper, all calls will be added as LLM spans to that trace.

<CodeGroup>
  <SnippetSdkWrappersOpenAIManualTracePython />
  <SnippetSdkWrappersOpenAIManualTraceTypeScript />
</CodeGroup>

![A single session called Chat about the Roman Empire with 1 trace in the Galileo Console](/sdk-api/third-party-integrations/openai/manual-session-one-trace.webp)

## Streaming support

The OpenAI wrapper also supports streaming responses. When streaming, the wrapper will log the response as it streams in:

<CodeGroup>
  <SnippetSdkWrappersOpenAIStreamingPython />
  <SnippetSdkWrappersOpenAIStreamingTypeScript />
</CodeGroup>

## Combining with the log decorator

You can combine the OpenAI wrapper with the `log` decorator to create more complex traces:

<CodeGroup>
  <SnippetSdkWrappersOpenAILogDecoratorPython />
  <SnippetSdkWrappersOpenAILogDecoratorTypeScript />
</CodeGroup>

## Benefits of using the OpenAI integration

- **Zero-config logging**: No need to add logging code throughout your application
- **Complete visibility**: All prompts and responses are automatically captured
- **Minimal code changes**: Change your import statement in Python, or create a wrapper in TypeScript. No other code changes are required.
- **Automatic tracing**: Creates spans and traces without manual setup
- **Streaming support**: Works with both regular and streaming responses

<Note>
### Asynchronous OpenAI calls with Galileo

Galileo's Python SDK includes an OpenAI wrapper that currently supports only synchronous calls through the OpenAI client. It currently doesn't not include built-in support for the `AsyncOpenAI` class from the official OpenAI Python library. As a result, asynchronous calls made via `galileo.openai` wrapper won't automatically generate LLM spans or upload telemetry to Galileo.

You can still track async interactions by manually using the low-level `GalileoLogger` API. This requires importing and awaiting the OpenAI `AsyncOpenAI` client, wrapping each call with [a call to add an LLM span](/sdk-api/logging/galileo-logger#llm-spans), and flushing the logger to send your traces.
</Note>

## Next steps

<CardGroup cols={2}>
<Card title="Galileo logger" icon="code" horizontal href="/sdk-api/logging/galileo-logger">
    Log with full control over sessions, traces, and spans using the Galileo logger.
</Card>
<Card title="Log decorator" icon="code" horizontal href="/sdk-api/logging/log-decorator/log-decorator">
    Quickly add logging to your code with the log decorator and wrapper.
</Card>
<Card title="Galileo context" icon="code" horizontal href="/sdk-api/logging/galileo-context">
    Manage logging using the Galileo context manager.
</Card>
</CardGroup>
