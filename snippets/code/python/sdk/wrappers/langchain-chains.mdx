```python Python
from galileo.handlers.langchain import GalileoAsyncCallback
from langchain_openai import ChatOpenAI
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate
from langchain.schema.runnable.config import RunnableConfig

# Create a callback handler
callback = GalileoAsyncCallback()

# Create the model
llm = ChatOpenAI(model="gpt-4o", temperature=0.7, callbacks=[callback])

# Create a prompt template
prompt = ChatPromptTemplate.from_template("Tell me a joke about {topic}")

# Assemble the chain with the prompt, LLM, and output parser
chain = prompt | llm | StrOutputParser()

# Create a configuration for the runnable
# that includes the callback handler
config = RunnableConfig(
    callbacks=[callback]
)

# Invoke the chain with a topic and configuration
response = chain.invoke({"topic": "the Roman Empire"},
                        config=config)
print(response)
```
