```typescript TypeScript
import { config } from "dotenv";
import { OpenAI } from "openai";
import { log, getLogger, wrapOpenAI, init } from "galileo";

// load env variables
config();

/* Optionally initialize the logger if you haven't set
   GALILEO_PROJECT and GALILEO_LOG_STREAM environment variables:
    
   await init({ 
      projectName: "my-project", 
      logStreamName: "my-log-stream"
   })
 */

const openai = wrapOpenAI(new OpenAI({ apiKey: process.env.OPENAI_API_KEY }));

// This will automatically create an llm span since we're using the `wrapOpenAI` wrapper
const callOpenAI = async (input: string) => {
  const result = await openai.chat.completions.create({
    model: "gpt-4o",
    messages: [{ content: input, role: "user" }]
  });
  return result.choices[0].message.content;
};

const makeNestedCall = log({ name: "workflow span" }, async () => {
  // Prepare our user input
  const input = "The Roman Empire";

  // Get the currently active logger
  const galileoLogger = getLogger();
  galileoLogger.addWorkflowSpan({ input });

  const firstResult = await callOpenAI(input);
  const secondResult = await callOpenAI(`Summarize this: ${firstResult}`);
  return secondResult;
});

// When called, this will create a trace with a workflow span and two nested LLM spans
async function main() {
  const response = await makeNestedCall();
  console.log(response);
}

main()
```