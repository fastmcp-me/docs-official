## Metrics Reference

The table below summarizes each metric's purpose and the exact slug to reference in the SDK.

| Metric                              | SDK Slug(s)                     | Description                                                                                       |
| :---------------------------------- | :------------------------------ | :-------------------------------------------------------------------------------------------------------- |
| [Action Advancement](/concepts/metrics/agentic/action-advancement)  | `agentic_workflow_success`            | Measures whether a step or span moves the user closer to their overall goal within the session.           |
| [Action Completion](/concepts/metrics/agentic/action-completion)  | `agentic_session_success`             | Assesses if the user's goal was ultimately achieved at the session or trace level.                        |
| [BLEU](/concepts/metrics/expression-and-readability/bleu-and-rouge#understanding-bleu-score)  | `bleu`             | BLEU is a case-sensitive measurement of the difference between a model generation and target generation at the sentence-level.                        |
| [Chunk Attribution](/concepts/metrics/response-quality/chunk-attribution)  | `chunk_attribution_utilization_gpt` | Measures whether or not each chunk retrieved in a RAG pipeline had an effect on the model's response. |
| [Chunk Utilization](/concepts/metrics/response-quality/chunk-utilization)  | `chunk_attribution_utilization_gpt` | Measures the fraction of text in each retrieved chunk that had an impact on the model's response in a RAG pipeline. |
| [Completeness](/concepts/metrics/response-quality/completeness)  | `completeness_gpt`                  | Assesses whether the response covers all necessary aspects of the prompt or question.                     |
| [Context Adherence](/concepts/metrics/response-quality/context-adherence)  | `context_adherence_gpt`             | Evaluates if the LLM output is consistent with and grounded in the provided context.                      |
| Context Relevance (Query Adherence) | `context_relevance`  | Measures whether the retrieved context has enough information to answer the user's query.  |
| [Correctness (factuality)](/concepts/metrics/response-quality/correctness) | `correctness`   | Evaluates whether the output is factually correct based on available information.                         |
| [Ground Truth Adherence](/concepts/metrics/response-quality/ground-truth-adherence)  | `ground_truth_adherence`        | Measures semantic equivalence between model output and ground truth, typically using LLM-based judgment.  |
| [Instruction Adherence](/concepts/metrics/response-quality/instruction-adherence)  | `instruction_adherence`         | Checks if the LLM output follows the explicit instructions given in the prompt.                           |
| [Prompt Injection](/concepts/metrics/safety-and-compliance/prompt-injection)  | `prompt_injection_gpt`          | Measures the presence of prompt injection attacks in inputs to the LLM.                                   |
| [Prompt Perplexity](/concepts/metrics/model-confidence/prompt-perplexity)  | `prompt_perplexity`             | Indicates how “surprising” or difficult the prompt is for the model.                                      |
| [ROUGE](/concepts/metrics/expression-and-readability/bleu-and-rouge#understanding-rouge)  | `rouge` | Measures the unigram overlap between model generation and target generation as a single F-1 score.   |
| [Sexism / Bias](/concepts/metrics/safety-and-compliance/sexism)  | `input_sexist_gpt`, `output_sexist_gpt` | Measures how 'sexist' an input or output might be perceived as a value between 0 and 1 (1 being more sexist).   |
| [Tone](/concepts/metrics/expression-and-readability/tone)  | `input_tone_gpt`, `output_tone_gpt`     | Detects the tone (e.g., polite, neutral, aggressive) of the input/output. |
| [Tool Errors](/concepts/metrics/agentic/tool-error)  | `tool_error_rate`               | Flags errors that occur when an agent or LLM calls a tool (e.g., API or function call fails). |
| [Tool Selection Quality](/concepts/metrics/agentic/tool-selection-quality) | `tool_selection_quality`        | Determines if the agent/LLM selected the correct tool(s) and provided appropriate arguments. |
| [Toxicity](/concepts/metrics/safety-and-compliance/toxicity)  | `input_toxicity_gpt`, `output_toxicity_gpt` | Measures the presence and severity of harmful, offensive, or abusive language |
