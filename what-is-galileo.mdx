---
title: What Is Galileo?
icon: "house"
---

import { DefinitionCard } from "/snippets/components/definition-card.mdx";

Galileo's evaluation and observability platform empowers developers to evaluate and improve their AI apps and agents. Use Python or Typescript SDKs to easily add evals directly into your code, gather insights, apply run-time guardrails, and improve AI reliability. 

<CardGroup cols={2}>
<Card title="Log your first trace" icon="code" horizontal href="/getting-started/quickstart">
    Learn how to log your first trace with Galileo
</Card>
<Card title="Evaluate your first trace" icon="code" horizontal href="/getting-started/evaluate-and-improve/evaluate-and-improve">
    Learn how to evaluating metrics for your first trace with Galileo, and improve your application
</Card>
<Card title="Sample projects" icon="code" horizontal href="/getting-started/sample-projects/sample-projects">
    Learn how to get started with the Galileo sample projects that are included in every new account.
</Card>
<Card title="Contact us" icon="comment" horizontal href="https://galileo.ai/contact-sales">
    Got questions? Contact us to schedule time to learn about our evaluation platform.
</Card>
</CardGroup>

Galileo simplifies this process by providing metrics to evaluate, improve, and continuously monitor the performance of your generative AI applications. With Galileo, teams can quickly identify blind spots, track changes in model behavior, and accelerate the development of reliable, high-quality AI solutions.

> **Stay up to date:** Check our [Release Notes](/release-notes) for the latest features and improvements.

## The challenge

<DefinitionCard>**AI applications** introduce a unique set of challenges that traditional testing methods _simply cannot address_.</DefinitionCard>

When building AI applications, even when you feed the exact same input into your system, you might receive a range of different outputs, complicating the process of defining what _**"correct"** even means_. This variability makes it difficult to establish consistent benchmarks and increases the complexity of debugging when something goes awry.

Moreover, as the underlying models and data are updated and evolve, application behavior can shift unexpectedly, rendering previously successful tests obsolete. This dynamic environment requires tools that not only measure performance accurately but also adapt to ongoing changes, all while providing clear, actionable insights
into the AI's behavior across its entire lifecycle.

## How Galileo helps

<CardGroup cols={2}>
  <Card title="Identify issues with powerful metrics" icon="magnifying-glass-chart" horizontal href="/concepts/metrics/overview">
    Pinpoint problems instantly with built-in and custom metrics. Get analytics across correctness, completeness, safety, and relevance dimensions. Use token-level highlighting to diagnose root causes and implement targeted fixes.

</Card>
  <Card title="Run experiments with structured datasets" icon="flask" horizontal href="/concepts/datasets">
    Evaluate your AI with organized datasets targeting specific scenarios and edge cases. Build regression test suites, compare performance across inputs, and track improvements over time to prevent regressions.

</Card>
  <Card title="Test and compare multiple approaches" icon="code-compare" horizontal href="/concepts/experiments/overview">
    Compare models, prompts, and configurations side-by-side with quantifiable metrics. Run controlled tests to measure the impact of changes and make data-driven decisions when optimizing your AI systems.

</Card>
  <Card title="Stop prompt attacks, data leaks, and hallucinations with runtime protection" icon="shield-halved" horizontal href="/concepts/protect/overview">
    Deploy runtime protection in production. Get immediate visibility into model behavior and set thresholds that maintain quality and safety in your live AI systems.

</Card>
</CardGroup>

## Features

Galileo delivers essential tools for AI development - from evaluation metrics and RAG-specific tools to a robust experimentation framework. Everything you need to build, test, and maintain high-quality AI systems throughout their lifecycle.

<Card icon="moon" title="Luna-2 Evaluation model" href="/concepts/luna/luna">
Discover Galileo's Luna-2 Evaluation model, reducing the latency and cost for metric evaluations.
</Card>

<CardGroup cols={2}>
  <Card title="Data-driven metrics" icon="chart-bar">
    Automated, token-level quality checks to reveal nuanced performance insights. Understand exactly how your AI is performing with detailed analytics.

</Card>
  <Card title="Configurable regression detection" icon="radar">
    Tolerance thresholds that filter out minor fluctuations, highlighting significant issues. Get alerted only when changes matter to your application.

</Card>
  <Card title="Integrated feedback" icon="comments">
    Seamlessly incorporates real-world insights into your development cycle. Turn user feedback into actionable improvements for your AI system.

</Card>
  <Card title="End-to-end visibility" icon="eye">
    Clear, visual tracking of your AI application's performanceâ€”from prompt design to production. Monitor the complete lifecycle in one unified interface.

</Card>
</CardGroup>

## Get started

<CardGroup cols={1}>
<Card title="Log your first trace" icon="code" horizontal href="/getting-started/quickstart">
    Learn how to log your first trace with Galileo
</Card>
<Card title="Log and evaluate your first trace" icon="code" horizontal href="/getting-started/evaluate-and-improve/evaluate-and-improve">
    Learn how to evaluating metrics for your trace with Galileo, and improve your application
</Card>
</CardGroup>
